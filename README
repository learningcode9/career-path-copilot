# Career Path Copilot ğŸš€  
*End-to-end Azure Data Engineering project using Medallion Architecture*

## Introduction  
This project demonstrates how to build a **Career Path Copilot** â€” an application that ingests a **Job Description (JD)** and a **Candidate Profile**, then prepares them for **skills matching, gap analysis, and personalized learning plans**.  

It is built on **Azure Data Engineering best practices**, following the **Medallion Architecture (Landing â†’ Bronze â†’ Silver â†’ Gold)**, and is fully automated with **event-driven pipelines** and **GitHub source control**.

## Highlights (TL;DR)  
- **Architecture:** Medallion pattern â€” Landing â†’ Bronze â†’ Silver â†’ Gold in ADLS Gen2.  
- **Ingestion:** ADF pipeline `pl_copy_single_file` moves JD & Profile JSONs from `landing/` to `bronze/`.  
- **Automation:** Event Grid trigger `trig_landing_bronze` runs automatically when a file lands.  
- **Parameterization:** `p_request_id` parameter supports single JD+Profile pair or batch mode.  
- **Parallelism:** JD & Profile copy activities run together to keep requests in sync.  
- **Security:** Managed Identity + Storage Blob Data Contributor role.  
- **Source Control:** GitHub integration â€” `main` (JSON artifacts) and `adf_publish` (ARM templates).  
- **Next steps:**  
  - Silver in Databricks (transform, clean, extract skills).  
  - Gold curated tables for skills matching & learning plan.  
  - Minimal UI (Azure Static Web Apps + Function).  
  - CI/CD with GitHub Actions.  

## Repository Structure  

career-path-copilot/
 â”œâ”€ infra/                  â†’ IaC (ARM/Bicep/Terraform if you want infra-as-code)
 â”œâ”€ adf/                    â†’ Exported ADF pipelines (JSON ARM templates)
 â”œâ”€ databricks/             â†’ Notebooks (PySpark, Delta Live Tables)
 â”œâ”€ functions/              â†’ Azure Functions (API code for frontend/backend glue)
 â”œâ”€ frontend/               â†’ React/Static Web App code
 â”œâ”€ docs/                   â†’ Architecture diagrams, screenshots, README guides
 â””â”€ .github/workflows/      â†’ GitHub Actions for CI/CD


- **main branch** â†’ collaboration branch, where JSON files for datasets, pipelines, and triggers are stored (source of truth).  
- **adf_publish branch** â†’ automatically created by ADF when you click *Publish*. Contains ARM templates used for CI/CD deployment.

---

## ğŸš€ Current Pipelines

- **pl_copy_single_file**
  - Copies Job Description (JD) and Profile JSONs from `landing/` into `bronze/`.
  - Parameterized with `p_request_id` so JD + Profile run together for the same request.

---

## âš¡ Triggers

- **trig_landing_bronze**
  - Blob event trigger that starts the pipeline when a new `.json` file is added to `/landing/jd/`.

---

## ğŸ—ï¸ Medallion Architecture

This project follows the Medallion data lake pattern:

- **Landing** â†’ Raw uploads (JD & Profile JSONs).  
- **Bronze** â†’ Ingested data, one-to-one copy from landing.  
- **Silver** â†’ Cleaned & transformed data (to be built next).  
- **Curated (Gold)** â†’ Final structured layer for analytics & UI.

---

## ğŸ”— CI/CD

- Use the **adf_publish** branch for deployments.
- `ARMTemplateForFactory.json` + `ARMTemplateParametersForFactory.json` enable automated deployment with Azure DevOps or GitHub Actions.

---

## âœ… Next Steps

- Build Databricks notebooks to transform **Bronze â†’ Silver**.
- Add `skills-matching` logic.
- Expose curated data for UI via Synapse / SQL Database.
